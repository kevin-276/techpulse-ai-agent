import os
import aiohttp 
import asyncio
import aiosqlite 
from typing import Annotated, Literal, TypedDict, List

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver 
from langgraph.graph.message import add_messages
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage

from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document # ðŸ‘ˆ æ–°å¢žï¼šç”¨äºŽæž„é€ å­˜å…¥æ•°æ®åº“çš„æ–‡æ¡£æ ¼å¼

# ==========================================
# 1. Prompt (ä¿æŒä¸å˜)
# ==========================================
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå…·å¤‡ã€è‡ªæˆ‘åæ€(Self-Reflection)ã€‘èƒ½åŠ›çš„ GitHub ä¸“å®¶ Agentã€‚
ä½ æ‹¥æœ‰ä¸¤ä¸ªå·¥å…·ï¼š
1. search_local_memoryï¼ˆæœ¬åœ°çŸ¥è¯†åº“ï¼‰
2. search_githubï¼ˆè”ç½‘æœç´¢ï¼‰

ã€ä½ çš„æ ‡å‡†å·¥ä½œæµã€‘ï¼š
æ­¥éª¤ 1: æ°¸è¿œå…ˆè°ƒç”¨ search_local_memory æ£€ç´¢æœ¬åœ°çŸ¥è¯†ã€‚
æ­¥éª¤ 2: ã€å…³é”®å†³ç­–ã€‘ä½œä¸ºè£åˆ¤ï¼Œå®¡è§†æœ¬åœ°å·¥å…·è¿”å›žçš„ç»“æžœã€‚åˆ©ç”¨ä½ çš„å¸¸è¯†åˆ¤æ–­ï¼šè¿”å›žçš„é¡¹ç›®å’Œç”¨æˆ·æƒ³æ‰¾çš„é¡¹ç›®æ˜¯åŒä¸€ä¸ªä¸œè¥¿å—ï¼Ÿ
æ­¥éª¤ 3: å¦‚æžœä½ è®¤ä¸ºæœ¬åœ°ç»“æžœæ˜¯â€œç­”éžæ‰€é—®â€ï¼ˆä¾‹å¦‚ç”¨æˆ·æ‰¾ OpenClawï¼Œå´è¿”å›žäº† AutoGPTï¼‰ï¼Œè¯´æ˜Žæœ¬åœ°æ²¡æœ‰è¯¥çŸ¥è¯†ã€‚æ­¤æ—¶ï¼Œä½ **å¿…é¡»ä¸»åŠ¨ã€é™é»˜åœ°**è°ƒç”¨ search_github å·¥å…·è”ç½‘æŸ¥è¯¢ã€‚ç»å¯¹ä¸è¦æŠŠé”™è¯¯çš„æœ¬åœ°ç»“æžœå‘Šè¯‰ç”¨æˆ·ï¼Œä¹Ÿä¸è¦å‘ç”¨æˆ·æé—®ã€‚
æ­¥éª¤ 4: åŸºäºŽæ­£ç¡®çš„æ•°æ®ï¼ˆæœ¬åœ°çš„æˆ–è”ç½‘æŸ¥åˆ°çš„ï¼‰ï¼Œå‘ç”¨æˆ·è¾“å‡ºæœ€ç»ˆå›žç­”ã€‚
"""

# ==========================================
# 2. å¼‚æ­¥ Agent æž„é€ å™¨ (æ ¸å¿ƒï¼šè‡ªåŠ¨å­¦ä¹ æœºåˆ¶)
# ==========================================
def build_graph(api_key: str):
    if not api_key: raise ValueError("API Key is missing")

    # 1. åˆå§‹åŒ– Embedding å’Œ å‘é‡æ•°æ®åº“
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/gemini-embedding-001",
        google_api_key=api_key 
    )

    local_vector_store = Chroma(
        persist_directory="chroma_db", 
        embedding_function=embeddings
    )

    # ðŸ”§ å·¥å…· Aï¼šæ£€ç´¢æœ¬åœ°è®°å¿† (Agentic è¯„åˆ¤ç‰ˆ)
    @tool
    async def search_local_memory(query: str):
        """ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·ï¼åœ¨æœ¬åœ°å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢é¡¹ç›®ä¿¡æ¯ã€‚"""
        print(f"--- [Backend] ðŸ§  æ­£åœ¨æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“: {query} ---")
        try:
            # ðŸ’¡ æ ¸å¿ƒå‡çº§ï¼šåŒæ—¶èŽ·å–ç›¸ä¼¼åº¦å¾—åˆ† (Score)
            # åœ¨ Chroma ä¸­ï¼Œé»˜è®¤ä½¿ç”¨ L2 è·ç¦»ï¼Œæ•°å€¼è¶Šå°ä»£è¡¨è¶Šç›¸ä¼¼
            results = await local_vector_store.asimilarity_search_with_score(query, k=2)
            
            if not results:
                return "æœ¬åœ°çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·ç«‹åˆ»è°ƒç”¨ search_github å·¥å…·è”ç½‘æŸ¥è¯¢ã€‚"
            
            info_list = []
            for doc, score in results:
                repo_name = doc.metadata.get('repo', 'Unknown')
                # æŠŠåˆ†æ•°ä¹Ÿå–‚ç»™å¤§æ¨¡åž‹ï¼Œè¾…åŠ©å®ƒåšå†³ç­–
                info_list.append(f"é¡¹ç›®: {repo_name}\næè¿°: {doc.page_content}\nå‘é‡è·ç¦»å¾—åˆ†(è¶ŠæŽ¥è¿‘0è¶ŠåŒ¹é…): {score:.2f}")
            
            # ðŸ’¡ æ ¸å¿ƒå‡çº§ï¼šåœ¨å·¥å…·è¿”å›žå€¼ä¸­ï¼Œæ¤å…¥â€œåæ€æŒ‡ä»¤â€
            return (
                "ã€æœ¬åœ°æ£€ç´¢ç»“æžœã€‘å¦‚ä¸‹ï¼š\n"
                "âš ï¸ è¯·ä½ ä½œä¸ºè£åˆ¤ï¼Œè¯„ä¼°ä»¥ä¸‹ç»“æžœæ˜¯å¦çœŸçš„ç¬¦åˆç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ã€‚\n"
                "å¦‚æžœè·ç¦»å¾—åˆ†è¿‡å¤§ï¼Œæˆ–è€…é¡¹ç›®æè¿°æ˜Žæ˜¾ä¸ç¬¦ï¼Œè¯·å¿½ç•¥æ­¤ä¿¡æ¯ï¼Œå¹¶ç«‹åˆ»è°ƒç”¨ search_github å·¥å…·ï¼\n\n"
                + "\n---\n".join(info_list)
            )
            
        except Exception as e:
            return f"æœ¬åœ°æ£€ç´¢å‘ç”Ÿé”™è¯¯: {e}ï¼Œè¯·æ”¹ç”¨ search_githubã€‚"
    # ðŸ”§ å·¥å…· Bï¼šè”ç½‘æœ GitHub å¹¶è‡ªåŠ¨å­¦ä¹  (è¯» + å†™)
    @tool
    async def search_github(query: str):
        """å½“æœ¬åœ°è®°å¿†æ‰¾ä¸åˆ°æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·æœç´¢ GitHub å¹¶è‡ªåŠ¨å­¦ä¹ æ–°çŸ¥è¯†ã€‚"""
        print(f"--- [Backend] ðŸŒ æ­£åœ¨å¯åŠ¨è”ç½‘æœç´¢: {query} ---")
        url = f"https://api.github.com/search/repositories?q={query}"
        # ä»ŽçŽ¯å¢ƒå˜é‡è¯»å– Tokenï¼Œå¦‚æžœæ²¡é…ç½®ï¼Œå°±æä¾›ä¸€ä¸ªç©ºå­—ç¬¦ä¸²é˜²æ­¢æŠ¥é”™
        # ðŸŒŸ ä¿®å¤ç‚¹ï¼šæ›´å®‰å…¨åœ°è¯»å–å’Œæ‹¼è£… Headers
        github_token = os.environ.get("GITHUB_TOKEN", "").strip()
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        if github_token:
            headers["Authorization"] = f"Bearer {github_token}"
            
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=10) as response:
                    if response.status != 200: return f"Error: Status {response.status}"
                    data = await response.json()
                    if 'items' not in data: return f"Error: {data}"
                    
                    results = []
                    docs_to_learn = [] # ç”¨äºŽå‡†å¤‡å­˜å…¥æ•°æ®åº“çš„åˆ—è¡¨
                    
                    for item in data['items'][:3]: # å–å‰ 3 ä¸ªé«˜è´¨é‡ç»“æžœ
                        repo_name = item['full_name']
                        stars = item['stargazers_count']
                        desc = item['description']
                        
                        # æž„é€ çº¯æ–‡æœ¬ä¿¡æ¯ç»™å¤§æ¨¡åž‹çœ‹
                        content_str = f"Name: {repo_name}, Stars: {stars}, Desc: {desc}"
                        results.append(content_str)
                        
                        # æž„é€  Document å¯¹è±¡ç»™å‘é‡æ•°æ®åº“åƒ
                        # æŠŠåå­—å’Œæè¿°æ‹¼åœ¨ä¸€èµ·ä½œä¸ºâ€œè¯­ä¹‰å†…å®¹â€ï¼ŒæŠŠä»“åº“åä½œä¸ºâ€œå…ƒæ•°æ®â€
                        doc = Document(
                            page_content=f"{repo_name} æ˜¯ä¸€ä¸ª GitHub é¡¹ç›®ã€‚æè¿°ï¼š{desc}",
                            metadata={"repo": repo_name}
                        )
                        docs_to_learn.append(doc)
                    
                    # ðŸŒŸ æ ¸å¿ƒåŠ¨ä½œï¼šè‡ªåŠ¨å­¦ä¹ ï¼å°†æ–°çŸ¥è¯†å†™å…¥ ChromaDB
                    if docs_to_learn:
                        print(f"--- [Backend] ðŸ’¾ æ­£åœ¨è‡ªåŠ¨å­¦ä¹ ï¼å°† {len(docs_to_learn)} ä¸ªæ–°é¡¹ç›®å†™å…¥æœ¬åœ°çŸ¥è¯†åº“ ---")
                        # å¼‚æ­¥æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“
                        await local_vector_store.aadd_documents(docs_to_learn)
                    
                    return "\n".join(results)
        except Exception as e:
            return f"Search Network Error: {e}"

    # --- åŽç»­ç»„è£…å›¾é€»è¾‘ä¿æŒä¸å˜ ---
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", google_api_key=api_key, temperature=0)
    tools = [search_local_memory, search_github]
    llm_with_tools = llm.bind_tools(tools)

    class AgentState(TypedDict):
        messages: Annotated[list, add_messages]

    def agent_node(state: AgentState):
        messages = state['messages']
        sys_msg = SystemMessage(content=SYSTEM_PROMPT)
        response = llm_with_tools.invoke([sys_msg] + messages)
        return {"messages": [response]}

    tool_node = ToolNode(tools)

    def should_continue(state: AgentState) -> Literal["tools", END]:
        if state['messages'][-1].tool_calls: return "tools"
        return END

    workflow = StateGraph(AgentState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges("agent", should_continue)
    workflow.add_edge("tools", "agent")
    
    return workflow

# ==========================================
# 3. è¾…åŠ©åŠŸèƒ½ (ä¿æŒä¸å˜)
# ==========================================
DB_PATH = "agent_memory.sqlite"

def clear_memory_sync(thread_id: str):
    import sqlite3
    # å¦‚æžœæ•°æ®åº“æ–‡ä»¶éƒ½ä¸å­˜åœ¨ï¼Œè¯´æ˜Žæœ¬æ¥å°±æ˜¯ç©ºçš„ï¼Œç›´æŽ¥è¿”å›žæˆåŠŸ
    if not os.path.exists(DB_PATH): 
        return True 
        
    try:
        with sqlite3.connect(DB_PATH) as conn:
            cursor = conn.cursor()
            # å…¼å®¹ LangGraph æ–°è€ç‰ˆæœ¬çš„ä¸åŒè¡¨å
            tables_to_clear = ["checkpoints", "checkpoint_blobs", "checkpoint_writes", "writes"]
            
            for table in tables_to_clear:
                try:
                    cursor.execute(f"DELETE FROM {table} WHERE thread_id = ?", (thread_id,))
                except sqlite3.OperationalError:
                    # å¦‚æžœæŸä¸ªè¡¨ä¸å­˜åœ¨ï¼Œå¿½ç•¥æŠ¥é”™ç»§ç»­åˆ ä¸‹ä¸€ä¸ª
                    pass 
            conn.commit()
        return True
    except Exception as e:
        print(f"Error clearing memory: {e}")
        return False

def get_existing_users_sync() -> List[str]:
    import sqlite3
    if not os.path.exists(DB_PATH): return []
    try:
        with sqlite3.connect(DB_PATH) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT DISTINCT thread_id FROM checkpoints")
            rows = cursor.fetchall()
            return [row[0] for row in rows]
    except Exception as e:
        print(f"Error reading users: {e}")
        return []